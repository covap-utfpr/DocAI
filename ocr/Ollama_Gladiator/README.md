# üß† Ollama Gladiator ‚Äî Benchmark de OCR com LLMs Multimodais via Ollama

O **Ollama Gladiator** √© um projeto de **OCR baseado em LLMs multimodais**, executados localmente via **Ollama**, com foco em **benchmark comparativo e an√°lise direta** entre modelos visuais.

A proposta √© colocar diferentes LLMs multimodais em uma ‚Äúarena‚Äù, processando a **mesma imagem** para avaliar **qualidade, consist√™ncia e estrutura do OCR** gerado, com sa√≠da padronizada.

Os modelos atualmente configurados para execu√ß√£o no script `ollama_extract.py` s√£o:

```python
MODELOS_INSTALADOS = [
  "llama3.2-vision:11b",
  "gemma3:4b",
  "gemma3:12b",
  "ministral-3:3b",
  "ministral-3:8b",
  "ministral-3:14b",
  "qwen3-vl:2b",
  "qwen3-vl:4b",
  "qwen3-vl:8b",
]
```
Outros modelos multimodais podem ser adicionados ou removidos facilmente conforme desejado.

Os modelos dispon√≠veis podem ser consultados em: https://ollama.com/search?c=vision

Basta modificar a lista `MODELOS_INSTALADOS`.

## üöÄ Objetivo

O objetivo do projeto √© **extrair texto de imagens utilizando modelos multimodais do Ollama**, for√ßando uma sa√≠da **padronizada e estruturada**.

Cada modelo processa a mesma entrada, permitindo avalia√ß√£o comparativa e gerando:

- Arquivo `.txt` com as linhas OCR detectadas
- Arquivo `.json` estruturado a partir do OCR
- Resultados organizados por imagem **e por modelo**

Tudo isso de forma automatizada e totalmente local.

## üß† Tecnologias e Estrutura

- **Ollama**: Execu√ß√£o local dos modelos LLM (API REST na porta `11434`)
- **LLMs Multimodais Visuais** (podem ser modificados conforme os modelos instalados localmente)
  - `gemma3 (4b e 12b)`
  - `llama3.2-vision (11b)`
  - `ministral-3 (3b, 8b e 14b)`
  - `qwen3-vl (2b, 4b e 8b)`
- **Prompt Engineering**: Controle r√≠gido de formato de sa√≠da OCR
- **NDJSON Streaming**: Processamento cont√≠nuo da resposta dos modelos
- **Python**: Orquestra√ß√£o, parsing e persist√™ncia dos resultados

## üõ†Ô∏è Instala√ß√£o

### 1Ô∏è‚É£ Pr√©-requisitos

- Python **3.10+**
- Ollama instalado e funcional (dispon√≠vel em https://ollama.com/download)
- Modelos multimodais previamente baixados
- Servi√ßo do Ollama rodando em background

Exemplo de ativa√ß√£o do Ollama:
```bash
ollama serve
```

### 2Ô∏è‚É£ Depend√™ncias Python

Instale as depend√™ncias do projeto:
```bash
pip install -r requirements.txt
```

## üñºÔ∏è Como usar

### 1Ô∏è‚É£ Preparando as imagens

Voc√™ pode fornecer:

- Um arquivo de imagem individual
- Um diret√≥rio contendo v√°rias imagens (recursivo)

Extens√µes suportadas:
```
.jpg .jpeg .png .bmp .tiff .webp
```

### 2Ô∏è‚É£ Execu√ß√£o

- ‚ñ∂Ô∏è Processamento por arquivo
```bash
python3 ollama_extract.py imagem.jpg
```

- ‚ñ∂Ô∏è Processamento por diret√≥rio
```bash
python3 ollama_extract.py pasta_de_imagens/
```

- ‚ñ∂Ô∏è M√∫ltiplas entradas
```bash
python3 ollama_extract.py imagem1.png imagem2.jpg pasta/
```

O script:

- Expande automaticamente os caminhos
- Processa cada imagem com todos os modelos configurados
- Salva os resultados separadamente por modelo

## ‚öôÔ∏è Funcionamento Interno

- A imagem √© convertida para Base64 para suportar o uso da API na porta 11434
- O modelo recebe:
  - Um prompt extremamente restritivo podendo ser personalizado (fun√ß√£o `extract_text_from_image`)
  - A imagem embutida
  - O retorno vem em NDJSON streaming

As respostas s√£o concatenadas e separadas por linha onde cada linha segue o formato:
```
OCR='texto', score=0.95, bbox=[x1,y1,x2,y2]
```
- O .txt √© salvo
- O .txt √© parseado em tokens
- Os tokens s√£o convertidos em .json

## üìÅ Estrutura dos Arquivos
```
ollama_extract.py          # Script principal (OCR multimodelo)
Modules/
‚îú‚îÄ‚îÄ config.py              # Parse dos arquivos OCR (.txt)
‚îú‚îÄ‚îÄ json_processing.py     # Limpeza, convers√£o e salvamento em JSON
‚îú‚îÄ‚îÄ path.py                # Gerenciamento de diret√≥rios
images/                    # (Opcional) Pasta de imagens para ser carregado
results/                   # Resultados gerados por imagem e modelo
```

Exemplo de sa√≠da:
```
results/
‚îú‚îÄ‚îÄ nota_fiscal_llama3.2-vision_ocr.txt
‚îú‚îÄ‚îÄ nota_fiscal_llama3.2-vision.json
‚îú‚îÄ‚îÄ nota_fiscal_gemma3_ocr.txt
‚îú‚îÄ‚îÄ nota_fiscal_gemma3.json
```

##  üìå Observa√ß√µes finais
    O projeto ainda est√° em desenvolvimento ‚Äî fique √† vontade para sugerir melhorias!
